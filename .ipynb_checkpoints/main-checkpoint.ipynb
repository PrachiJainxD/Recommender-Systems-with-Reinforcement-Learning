{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7532ab77-0a4b-4916-88b0-f593bab7e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from mixture_model import MixtureModel\n",
    "from mdp import MDP\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832b7d71-962d-4f8a-b92d-a249211258e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "PATH='dataset'\n",
    "MIXTURE_MODEL_PATH='mixture-models'\n",
    "HEADERS = ['Rank', 'Game', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0fff3c-2f3d-46ff-a555-33c059047dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "def evaluate_model(user_id, model_load_path, k):\n",
    "    user_id = str(user_id)\n",
    "    mm = MDP(path=PATH)\n",
    "    mm.load(model_load_path+'/mdp-model_k='+str(k)+'.pkl')\n",
    "    r_list = mm.recommend(user_id) \n",
    "    print(tabulate([[ind+1, r[0], r[1]] for ind, r in enumerate(r_list)], HEADERS, \"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31a53b-35f5-4672-87d8-755c10045598",
   "metadata": {},
   "source": [
    "## Initialize MDP ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89868740-7089-4554-ae84-54349a5cc97c",
   "metadata": {},
   "source": [
    "## Mixture Models with Policy Iteration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d788bbd3-488c-4be6-b957-63074f67b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Mixture Model\n",
    "mixture_model = MixtureModel(path='dataset', k=3, verbose=False, save_path='saved_models/'+MIXTURE_MODEL_PATH)\n",
    "mixture_model.generate_model(max_iteration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39300a3d-b0b9-46be-a33b-eb14ca527251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from mixture-models/mdp-model_k=3.pkl\n",
      "+--------+---------------------------------+---------+\n",
      "|   Rank | Game                            |   Score |\n",
      "|--------+---------------------------------+---------|\n",
      "|      1 | Half-Life 2                     | 6.30681 |\n",
      "|      2 | Counter-Strike Condition Zero   | 6.30398 |\n",
      "|      3 | Robocraft                       | 6.30266 |\n",
      "|      4 | Half-Life 2 Deathmatch          | 6.30175 |\n",
      "|      5 | Portal 2                        | 6.29984 |\n",
      "|      6 | The Elder Scrolls V Skyrim      | 6.2995  |\n",
      "|      7 | Counter-Strike Source           | 6.29143 |\n",
      "|      8 | Left 4 Dead 2                   | 6.29054 |\n",
      "|      9 | Half-Life 2 Lost Coast          | 6.28785 |\n",
      "|     10 | Terraria                        | 6.28159 |\n",
      "|     11 | Sid Meier's Civilization V      | 6.27411 |\n",
      "|     12 | Garry's Mod                     | 6.26949 |\n",
      "|     13 | Counter-Strike                  | 6.26516 |\n",
      "|     14 | Portal                          | 6.26119 |\n",
      "|     15 | Heroes & Generals               | 6.17798 |\n",
      "|     16 | Unturned                        | 6.1586  |\n",
      "|     17 | Team Fortress 2                 | 6.06817 |\n",
      "|     18 | Warframe                        | 6.05613 |\n",
      "|     19 | Counter-Strike Global Offensive | 5.94756 |\n",
      "|     20 | Dota 2                          | 5.72587 |\n",
      "+--------+---------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(187131847, MIXTURE_MODEL_PATH, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41eca0f4-aa30-4c42-bbd3-b53acb051024",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Testing with more Iterations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mixture_model \u001b[38;5;241m=\u001b[39m MixtureModel(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_models/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mMIXTURE_MODEL_PATH)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmixture_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/687-reinforcement-learning/project/Recommender-Systems-with-Reinforcement-Learning/mixture_model.py:41\u001b[0m, in \u001b[0;36mMixtureModel.generate_model\u001b[0;34m(self, max_iteration)\u001b[0m\n\u001b[1;32m     39\u001b[0m mm\u001b[38;5;241m.\u001b[39minitialise_mdp()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Run the policy iteration and save the model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iteration\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/687-reinforcement-learning/project/Recommender-Systems-with-Reinforcement-Learning/mdp.py:159\u001b[0m, in \u001b[0;36mMDP.policy_iteration\u001b[0;34m(self, max_iteration, start_where_left_off, to_save)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_eval()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Improve policy\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# If the policy not changed over 10 iterations it converged\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/687-reinforcement-learning/project/Recommender-Systems-with-Reinforcement-Learning/mdp.py:100\u001b[0m, in \u001b[0;36mMDP.update_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_step_lookahead(state)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# The action with the highest action value is chosen\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[state] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_list[state] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(action_values\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m kv: kv[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing with more Iterations\n",
    "mixture_model = MixtureModel(path='dataset', k=3, verbose=True, save_path='saved_models/'+MIXTURE_MODEL_PATH)\n",
    "mixture_model.generate_model(max_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef138fff-df68-46e5-a456-eeaa4c75cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(187131847, MIXTURE_MODEL_PATH, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629ba33-c33a-4bc7-b286-6972ca357799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:responsible-ai-baseenv] *",
   "language": "python",
   "name": "conda-env-responsible-ai-baseenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
